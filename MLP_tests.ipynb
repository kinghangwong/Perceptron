{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208bea35-dd09-4c44-9c0d-b766f4f3e964",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9888462f-e5d2-40a8-a162-aa7ec3a02d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b78ea0-63c0-4a19-9ee9-ca2787b157f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= 'diabetes_scale.txt'\n",
    "\n",
    "# Read the data from the file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# Create a DataFrame\n",
    "col_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "       'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "df = pd.DataFrame([line.strip().split() for line in data], columns=[\"Outcome\"] + col_names)\n",
    "\n",
    "# Split each feature column into two columns: Feature ID and Feature Value\n",
    "for col in col_names:\n",
    "    feature_data = df[col].str.split(\":\", expand=True)\n",
    "    df[col] = feature_data[1].astype(float)\n",
    "\n",
    "df[\"Outcome\"] = df[\"Outcome\"].astype(int)\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract features and groundtruth columns\n",
    "X = df.iloc[:,1:].values\n",
    "y = df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff4a7d44-8840-4bdb-a6d3-22de7f19d132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((607, 8), (76, 8), (76, 8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_test, X_val, y_test, y_val  = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aae2306-bbc0-45d7-aad4-d40fc0bbf917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154649/1065188730.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "/tmp/ipykernel_154649/1065188730.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "# def build_tensor_data(X_train, X_test, X_val, y_train, y_test, y_val):\n",
    "#     '''\n",
    "#     Convert our data into 32-bit tensors\n",
    "#     '''\n",
    "#     X_test = torch.tensor(X_test).to(dtype=torch.float32)\n",
    "#     X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "#     X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "#     # map from [-1, 1] to [0, 1]\n",
    "#     if y_train.min() < 0:\n",
    "#         y_train = (y_train + 1) / 2\n",
    "#         y_test = (y_test + 1) / 2\n",
    "#         y_val = (y_val + 1) / 2\n",
    "\n",
    "#     y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "#     y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "#     y_val = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "#     return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "# X_train, X_test, X_val, y_train, y_test, y_val = build_tensor_data(X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "\n",
    "\n",
    "def build_tensor(X, y):\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    if y.min() < 0:\n",
    "        y = (y + 1) / 2\n",
    "    y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = build_tensor(X_train, y_train)\n",
    "X_test, y_test = build_tensor(X_test, y_test)\n",
    "X_val, y_val = build_tensor(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4134ea1-48a8-4d57-a31b-8d1ff687e76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Dataset(torch.utils.data.Dataset):\n",
    "#     'Characterizes a dataset for PyTorch'\n",
    "#     def __init__(self, features, labels):\n",
    "#         'Initialization'\n",
    "#         self.labels = labels\n",
    "#         self.features = features\n",
    "\n",
    "#     def __len__(self):\n",
    "#         'Denotes the total number of samples'\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         'Generates one sample of data'\n",
    "#         return self.features[i], self.labels[i]\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "    \n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset( X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36034f4-d36e-4f7b-b268-99fe08edeef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the dataloader class simply by passing the custom dataset object we created\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# demo / test -- this returns the first \"shuffled\" entry in the dataset\n",
    "for images, labels in train_dataloader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cddd496-ad65-4ff9-af9a-bd125ffd7954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def metric_eval(y_true, y_pred, isPrint=False):\n",
    "    \n",
    "    # y_true = y_true.detach().numpy()\n",
    "    # y_pred = y_pred.detach().numpy()\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if isPrint:\n",
    "\n",
    "        print(f'Precision: {precision:.2f}')\n",
    "        print(f'Recall: {recall:.2f}')\n",
    "        print(f'F1-score: {f1:.2f}')\n",
    "\n",
    "        print('Confusion Matrix:')\n",
    "        print(conf_matrix)\n",
    "    \n",
    "    return precision, recall, f1, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c1da6-7d6b-4fec-8987-28e81ee1189c",
   "metadata": {},
   "source": [
    "The training loop is simple.\n",
    "First we reset the gradients so that we don't use the gradients from the previous rows.  \n",
    "Next we predict labels using a batch from the dataloader and evaluate the loss.\n",
    "Then we backpropagate the loss by calling `loss.backward`, and tell the optimizer to step towards the minimum with that loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d141c7a-8dea-47d6-aff7-c492cf9cd6d8",
   "metadata": {},
   "source": [
    "## MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f8989a-dacf-4462-8516-8951aa0043be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=8, hidden_size1=64, hidden_size2=64, output_size=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        self.sigmoid =  nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = MLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7959502-f6c2-4e16-b87e-383474e9e805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataset, loss_fn, optimizer, num_epochs, learning_rate = 1, show_metrics = True):\n",
    "    \n",
    "    best_acc = -np.inf\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:  # batch-wise\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            break\n",
    "            \n",
    "        # Print the average loss for this epoch\n",
    "        # print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}')\n",
    "        \n",
    "        X_val, y_val = val_dataset.tensors[0], val_dataset.tensors[1]\n",
    "        y_pred = model(X_val)\n",
    "        y_pred = (y_pred >= 0.5).int()\n",
    "        # print(y_pred[:10], y_val[:10])\n",
    "        acc = (y_pred == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        # print(f'Accuracy {acc}')\n",
    "        ## Other metrics -\n",
    "        if show_metrics == True:\n",
    "            metric_eval(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "371920e2-7697-46d9-a6c7-95c6ce72d744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "hidden_size1 = 300\n",
    "hidden_size2 = 100\n",
    "output_size = 1\n",
    "\n",
    "mlp = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "learning_rate = 0.01\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(),lr=learning_rate)\n",
    "\n",
    "train_model(mlp, train_dataloader, val_dataset, loss_fn, optimizer, num_epochs=30 , show_metrics = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff2636-8514-4e12-a58f-17e00c8b3929",
   "metadata": {},
   "source": [
    "Experimentation with MLP\n",
    "\n",
    "* increase num of layers\n",
    "* change activation function\n",
    "* change learning rate\n",
    "* change weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac485418-0f47-4204-8117-b6f44163e929",
   "metadata": {},
   "source": [
    "### Use 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbaff3a9-1f38-405e-9ff8-d79304a073ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def MLPCrossValidationTest(X, y, learning_rate = 1, num_epochs = 100):\n",
    "    \n",
    "    # init historical metric arrays\n",
    "    acc_arr, p_arr, r_arr, f1_arr = [], [], [], []\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    for train_index, val_index in kf.split(X):\n",
    "        # print(\"TRAIN:\", train_index, \"VAL:\", val_index)\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        # convert to [0, 1] and into tensor datatype\n",
    "        X_train, y_train = build_tensor(X_train, y_train) \n",
    "        X_val, y_val = build_tensor(X_val, y_val)\n",
    "        \n",
    "        # create datasets and dataloader\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle=True)\n",
    "        \n",
    "        ##########\n",
    "        ########## Adjust layers here\n",
    "        mlp = MLP(input_size=8, hidden_size1=256, hidden_size2=256, output_size=1)\n",
    "        \n",
    "        # model params\n",
    "        learning_rate = 0.01\n",
    "        loss_fn = nn.BCELoss()\n",
    "        optimizer = torch.optim.SGD(mlp.parameters(),lr=learning_rate)\n",
    "\n",
    "        # train model\n",
    "        train_model(mlp, train_dataloader, val_dataset, loss_fn, optimizer, num_epochs=num_epochs, learning_rate = learning_rate, show_metrics = False)\n",
    "        \n",
    "        # validation\n",
    "        X_val = val_dataset.tensors[0]\n",
    "        y_val = val_dataset.tensors[1]\n",
    "        \n",
    "        predictions = mlp(X_val)\n",
    "        predictions = (predictions >= 0.5).int()\n",
    "        \n",
    "        test_acc = accuracy_score(predictions, y_val)  # again, val <-> test here\n",
    "        # print(f'Test acc for learning rate {learning_rate}',test_acc)\n",
    "        p, r, f1, _ = metric_eval(predictions, y_val)\n",
    "        \n",
    "        acc_arr.append(test_acc)\n",
    "        p_arr.append(p)\n",
    "        r_arr.append(r)\n",
    "        f1_arr.append(f1)\n",
    "        \n",
    "    return acc_arr, p_arr, r_arr, f1_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64528e55-6e14-45a0-b397-d93f5188091a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc, precision, recall, f1\n",
      "lr = 1 :  [0.7325984663645869, 0.8721080275419798, 0.7553931073842579, 0.808435408157125]\n",
      "acc, precision, recall, f1\n",
      "lr = 0.1 :  [0.7443795747647263, 0.8897922358023186, 0.766344573602986, 0.8192576232094572]\n",
      "acc, precision, recall, f1\n",
      "lr = 0.01 :  [0.7457215057511328, 0.8838491444129287, 0.7656109503035514, 0.8183189245968471]\n",
      "acc, precision, recall, f1\n",
      "lr = 0.001 :  [0.7377831997211572, 0.8880790012393277, 0.7565072614324857, 0.8148051701262965]\n"
     ]
    }
   ],
   "source": [
    "# nodes are 8, 64, 64, 1\n",
    "\n",
    "for learn_r in [1, 0.1, 0.01, 0.001]:\n",
    "\n",
    "    acc_arr, p_arr, r_arr, f1_arr = MLPCrossValidationTest(X, y, learning_rate = learn_r)\n",
    "    print('acc, precision, recall, f1')\n",
    "    print(f'lr = {learn_r} : ', [np.mean(x) for x in [acc_arr, p_arr, r_arr, f1_arr] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f3e7b-8df6-4341-b074-fd9adf0f613c",
   "metadata": {},
   "source": [
    "It would seem learning rate has little effect given number of epochs = 100 (which is what we used for SLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8aee3-7dd2-414c-ad95-9eb859f1f344",
   "metadata": {},
   "source": [
    "## Try networks of various widths\n",
    "\n",
    "I adjusted MLP width settings inside of the MLPCrossValidationTest function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acffd108-bd08-4dba-a61c-3499b7bdde73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc, precision, recall, f1\n",
      "lr = 0.01 :  [0.6864761240850471, 0.7752677777903066, 0.7656477258720572, 0.7557518309535288]\n"
     ]
    }
   ],
   "source": [
    "# for 8, 8, 8, 1\n",
    "lr = 0.01\n",
    "acc_arr, p_arr, r_arr, f1_arr = MLPCrossValidationTest(X, y, learning_rate = lr)\n",
    "print('acc, precision, recall, f1')\n",
    "print(f'lr = {lr} : ', [np.mean(x) for x in [acc_arr, p_arr, r_arr, f1_arr] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d6fff00-7910-4cbe-80c2-2c5b8dacf82a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc, precision, recall, f1\n",
      "lr = 0.01 :  [0.6205820843499477, 0.8581298454479569, 0.6638457732691339, 0.743868671210518]\n"
     ]
    }
   ],
   "source": [
    "# 8, 4, 4, 1\n",
    "lr = 0.01\n",
    "acc_arr, p_arr, r_arr, f1_arr = MLPCrossValidationTest(X, y, learning_rate = lr)\n",
    "print('acc, precision, recall, f1')\n",
    "print(f'lr = {lr} : ', [np.mean(x) for x in [acc_arr, p_arr, r_arr, f1_arr] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "680e2483-ca64-4eca-9900-0e916e2b2030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc, precision, recall, f1\n",
      "lr = 0.01 :  [0.7681334959916348, 0.8635565052397212, 0.7991670538542021, 0.8284484933339848]\n"
     ]
    }
   ],
   "source": [
    "# 8, 256, 256, 1\n",
    "lr = 0.01\n",
    "acc_arr, p_arr, r_arr, f1_arr = MLPCrossValidationTest(X, y, learning_rate = lr)\n",
    "print('acc, precision, recall, f1')\n",
    "print(f'lr = {lr} : ', [np.mean(x) for x in [acc_arr, p_arr, r_arr, f1_arr] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a4630-3560-4130-800a-2d0b48b112df",
   "metadata": {},
   "source": [
    "MLP with different weight initialization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
